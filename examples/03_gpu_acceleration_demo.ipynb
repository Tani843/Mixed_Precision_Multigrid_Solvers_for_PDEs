{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: GPU Acceleration Demo\n",
    "\n",
    "## High-Performance Computing with CUDA and Multi-GPU\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand GPU computing advantages for multigrid methods\n",
    "- Compare CPU vs GPU performance for large-scale problems\n",
    "- Explore custom CUDA kernels for multigrid operations\n",
    "- Learn multi-GPU domain decomposition strategies\n",
    "- Analyze memory and computational performance scaling\n",
    "\n",
    "**Prerequisites:** Tutorials 1-2, basic understanding of parallel computing\n",
    "\n",
    "**Hardware Requirements:** NVIDIA GPU with CUDA support (optional - CPU fallback available)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: GPU Computing Fundamentals for Multigrid\n",
    "\n",
    "### Why GPUs for Multigrid Methods?\n",
    "\n",
    "**GPU Advantages:**\n",
    "- **Massive Parallelism**: Thousands of cores for simultaneous operations\n",
    "- **High Memory Bandwidth**: Fast data access for stencil operations\n",
    "- **SIMD Architecture**: Perfect for uniform grid computations\n",
    "- **Specialized Hardware**: Tensor cores for mixed-precision operations\n",
    "\n",
    "**Multigrid-Specific Benefits:**\n",
    "- **Smoothing Operations**: Highly parallelizable point-wise updates\n",
    "- **Grid Transfer**: Parallel restriction and prolongation\n",
    "- **Memory Locality**: Efficient shared memory usage for stencils\n",
    "- **Mixed Precision**: Hardware-accelerated precision switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and check GPU availability\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "import psutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to Python path\n",
    "sys.path.insert(0, str(Path('.').parent / \"src\"))\n",
    "\n",
    "# Try to import CuPy for GPU computing\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"âœ… CuPy (GPU support) available\")\n",
    "    \n",
    "    # Get GPU information\n",
    "    gpu_device = cp.cuda.Device(0)\n",
    "    gpu_properties = gpu_device.attributes\n",
    "    gpu_memory = gpu_device.mem_info\n",
    "    \n",
    "    print(f\"ðŸ–¥ï¸  GPU Device: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "    print(f\"   Compute Capability: {gpu_device.compute_capability}\")\n",
    "    print(f\"   Total Memory: {gpu_memory[1] / 1e9:.1f} GB\")\n",
    "    print(f\"   Free Memory: {gpu_memory[0] / 1e9:.1f} GB\")\n",
    "    print(f\"   SM Count: {gpu_properties['MultiProcessorCount']}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  CuPy not available - using CPU-only mode\")\n",
    "    print(\"   Install CuPy for full GPU acceleration: pip install cupy-cuda11x\")\n",
    "    GPU_AVAILABLE = False\n",
    "    # Create dummy cupy module for CPU fallback\n",
    "    class DummyCupy:\n",
    "        def array(self, x): return np.array(x)\n",
    "        def zeros_like(self, x): return np.zeros_like(x)\n",
    "        def asarray(self, x): return np.asarray(x)\n",
    "    cp = DummyCupy()\n",
    "\n",
    "# Import our multigrid components\n",
    "from multigrid.core.grid import Grid2D\n",
    "from multigrid.core.precision import PrecisionLevel\n",
    "from multigrid.solvers.mixed_precision_solver import MixedPrecisionMultigridSolver\n",
    "\n",
    "# Try to import GPU-specific components\n",
    "try:\n",
    "    from multigrid.gpu.cuda_kernels import SmoothingKernels, TransferKernels, MixedPrecisionKernels\n",
    "    from multigrid.gpu.multi_gpu_solver import MultiGPUSolver\n",
    "    print(\"âœ… GPU multigrid components loaded\")\n",
    "    GPU_MULTIGRID_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  GPU multigrid components not available - using CPU-only demonstrations\")\n",
    "    GPU_MULTIGRID_AVAILABLE = False\n",
    "\n",
    "# System information\n",
    "print(f\"\\nðŸ’» System Information:\")\n",
    "print(f\"   CPU: {psutil.cpu_count()} cores\")\n",
    "print(f\"   RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"\\nðŸš€ Ready for GPU acceleration demo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: CPU vs GPU Performance Comparison\n",
    "\n",
    "Let's start with a basic performance comparison for fundamental operations used in multigrid methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPerformanceComparison:\n",
    "    \"\"\"Compare CPU vs GPU for basic operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_array_operations(self, sizes):\n",
    "        \"\"\"Benchmark basic array operations.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¬ Benchmarking Array Operations (CPU vs GPU)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        operations = {\n",
    "            'creation': 'Array Creation',\n",
    "            'addition': 'Element-wise Addition', \n",
    "            'multiplication': 'Element-wise Multiplication',\n",
    "            'reduction': 'Sum Reduction',\n",
    "            'stencil': '5-point Stencil Operation'\n",
    "        }\n",
    "        \n",
    "        for size in sizes:\n",
    "            print(f\"\\nðŸ“ Testing {size}Ã—{size} arrays:\")\n",
    "            \n",
    "            # CPU operations\n",
    "            cpu_times = {}\n",
    "            \n",
    "            # Array creation\n",
    "            start = time.time()\n",
    "            cpu_a = np.random.random((size, size)).astype(np.float32)\n",
    "            cpu_b = np.random.random((size, size)).astype(np.float32)\n",
    "            cpu_times['creation'] = time.time() - start\n",
    "            \n",
    "            # Addition\n",
    "            start = time.time()\n",
    "            cpu_c = cpu_a + cpu_b\n",
    "            cpu_times['addition'] = time.time() - start\n",
    "            \n",
    "            # Multiplication\n",
    "            start = time.time()\n",
    "            cpu_d = cpu_a * cpu_b\n",
    "            cpu_times['multiplication'] = time.time() - start\n",
    "            \n",
    "            # Reduction\n",
    "            start = time.time()\n",
    "            cpu_sum = np.sum(cpu_a)\n",
    "            cpu_times['reduction'] = time.time() - start\n",
    "            \n",
    "            # 5-point stencil\n",
    "            start = time.time()\n",
    "            cpu_stencil = self._apply_stencil_cpu(cpu_a)\n",
    "            cpu_times['stencil'] = time.time() - start\n",
    "            \n",
    "            # GPU operations (if available)\n",
    "            gpu_times = {}\n",
    "            if GPU_AVAILABLE:\n",
    "                # Array creation and transfer\n",
    "                start = time.time()\n",
    "                gpu_a = cp.asarray(cpu_a)\n",
    "                gpu_b = cp.asarray(cpu_b)\n",
    "                cp.cuda.Device().synchronize()  # Ensure completion\n",
    "                gpu_times['creation'] = time.time() - start\n",
    "                \n",
    "                # Addition\n",
    "                start = time.time()\n",
    "                gpu_c = gpu_a + gpu_b\n",
    "                cp.cuda.Device().synchronize()\n",
    "                gpu_times['addition'] = time.time() - start\n",
    "                \n",
    "                # Multiplication\n",
    "                start = time.time()\n",
    "                gpu_d = gpu_a * gpu_b\n",
    "                cp.cuda.Device().synchronize()\n",
    "                gpu_times['multiplication'] = time.time() - start\n",
    "                \n",
    "                # Reduction\n",
    "                start = time.time()\n",
    "                gpu_sum = cp.sum(gpu_a)\n",
    "                cp.cuda.Device().synchronize()\n",
    "                gpu_times['reduction'] = time.time() - start\n",
    "                \n",
    "                # 5-point stencil\n",
    "                start = time.time()\n",
    "                gpu_stencil = self._apply_stencil_gpu(gpu_a)\n",
    "                cp.cuda.Device().synchronize()\n",
    "                gpu_times['stencil'] = time.time() - start\n",
    "                \n",
    "                # Verify results match\n",
    "                cpu_result = float(cpu_sum)\n",
    "                gpu_result = float(cp.asnumpy(gpu_sum))\n",
    "                if abs(cpu_result - gpu_result) > 1e-5:\n",
    "                    print(f\"   âš ï¸  Warning: CPU/GPU results differ: {cpu_result:.6f} vs {gpu_result:.6f}\")\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"   {'Operation':25s} {'CPU Time':>12s} {'GPU Time':>12s} {'Speedup':>10s}\")\n",
    "            print(f\"   {'-'*60}\")\n",
    "            \n",
    "            for op_key, op_name in operations.items():\n",
    "                cpu_time = cpu_times.get(op_key, 0)\n",
    "                gpu_time = gpu_times.get(op_key, 0) if GPU_AVAILABLE else 0\n",
    "                speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "                \n",
    "                cpu_str = f\"{cpu_time*1000:.2f} ms\"\n",
    "                gpu_str = f\"{gpu_time*1000:.2f} ms\" if GPU_AVAILABLE else \"N/A\"\n",
    "                speedup_str = f\"{speedup:.1f}Ã—\" if speedup > 0 else \"N/A\"\n",
    "                \n",
    "                print(f\"   {op_name:25s} {cpu_str:>12s} {gpu_str:>12s} {speedup_str:>10s}\")\n",
    "            \n",
    "            # Store results\n",
    "            self.results[size] = {\n",
    "                'cpu_times': cpu_times,\n",
    "                'gpu_times': gpu_times if GPU_AVAILABLE else {},\n",
    "                'data_size_mb': size * size * 4 / 1e6  # Single precision float\n",
    "            }\n",
    "    \n",
    "    def _apply_stencil_cpu(self, array):\n",
    "        \"\"\"Apply 5-point stencil on CPU.\"\"\"\n",
    "        result = np.zeros_like(array)\n",
    "        result[1:-1, 1:-1] = (array[1:-1, 1:-1] * 4 - \n",
    "                             array[:-2, 1:-1] - array[2:, 1:-1] -\n",
    "                             array[1:-1, :-2] - array[1:-1, 2:])\n",
    "        return result\n",
    "    \n",
    "    def _apply_stencil_gpu(self, gpu_array):\n",
    "        \"\"\"Apply 5-point stencil on GPU.\"\"\"\n",
    "        result = cp.zeros_like(gpu_array)\n",
    "        result[1:-1, 1:-1] = (gpu_array[1:-1, 1:-1] * 4 - \n",
    "                             gpu_array[:-2, 1:-1] - gpu_array[2:, 1:-1] -\n",
    "                             gpu_array[1:-1, :-2] - gpu_array[1:-1, 2:])\n",
    "        return result\n",
    "\n",
    "# Run performance comparison\n",
    "benchmark = BasicPerformanceComparison()\n",
    "test_sizes = [128, 256, 512, 1024] if GPU_AVAILABLE else [128, 256, 512]\n",
    "\n",
    "benchmark.benchmark_array_operations(test_sizes)\n",
    "\n",
    "print(\"\\nðŸ“Š Performance analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Custom CUDA Kernels for Multigrid Operations\n",
    "\n",
    "Now let's demonstrate specialized CUDA kernels optimized for multigrid operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CUDA kernels demonstration (CPU fallback if GPU not available)\n",
    "class MultigridCUDAKernelDemo:\n",
    "    \"\"\"Demonstrate custom CUDA kernels for multigrid operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_available = GPU_AVAILABLE and GPU_MULTIGRID_AVAILABLE\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            try:\n",
    "                self.smoothing_kernels = SmoothingKernels()\n",
    "                self.transfer_kernels = TransferKernels()\n",
    "                self.mixed_precision_kernels = MixedPrecisionKernels()\n",
    "                print(\"âœ… Custom CUDA kernels initialized\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  CUDA kernel initialization failed: {e}\")\n",
    "                self.gpu_available = False\n",
    "        \n",
    "        if not self.gpu_available:\n",
    "            print(\"ðŸ“ Using CPU implementations for demonstration\")\n",
    "    \n",
    "    def demonstrate_smoothing_kernels(self, grid_size=512):\n",
    "        \"\"\"Demonstrate different smoothing kernels.\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Smoothing Kernel Demonstration ({grid_size}Ã—{grid_size})\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create test problem\n",
    "        h = 1.0 / (grid_size - 1)\n",
    "        x = np.linspace(0, 1, grid_size)\n",
    "        y = np.linspace(0, 1, grid_size)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Right-hand side: f = 2Ï€Â²sin(Ï€x)sin(Ï€y)\n",
    "        rhs = 2 * np.pi**2 * np.sin(np.pi * X) * np.sin(np.pi * Y)\n",
    "        \n",
    "        # Initial guess\n",
    "        u_init = np.random.random((grid_size, grid_size)).astype(np.float32) * 0.1\n",
    "        \n",
    "        smoothing_methods = {\n",
    "            'jacobi': 'Jacobi Smoothing',\n",
    "            'gauss_seidel': 'Gauss-Seidel Smoothing',\n",
    "            'red_black_gs': 'Red-Black Gauss-Seidel',\n",
    "            'sor': 'Successive Over-Relaxation (SOR)'\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method, name in smoothing_methods.items():\n",
    "            print(f\"\\nðŸ”§ Testing {name}...\")\n",
    "            \n",
    "            # CPU implementation\n",
    "            u_cpu = u_init.copy()\n",
    "            start_cpu = time.time()\n",
    "            u_cpu = self._smooth_cpu(u_cpu, rhs, h, method, iterations=10)\n",
    "            cpu_time = time.time() - start_cpu\n",
    "            \n",
    "            cpu_residual = self._compute_residual(u_cpu, rhs, h)\n",
    "            \n",
    "            results[method] = {\n",
    "                'name': name,\n",
    "                'cpu_time': cpu_time,\n",
    "                'cpu_residual': cpu_residual,\n",
    "                'cpu_solution': u_cpu.copy()\n",
    "            }\n",
    "            \n",
    "            # GPU implementation (if available)\n",
    "            if self.gpu_available:\n",
    "                try:\n",
    "                    u_gpu = cp.asarray(u_init)\n",
    "                    rhs_gpu = cp.asarray(rhs.astype(np.float32))\n",
    "                    \n",
    "                    start_gpu = time.time()\n",
    "                    if method == 'jacobi':\n",
    "                        u_gpu = self.smoothing_kernels.jacobi_smooth(u_gpu, rhs_gpu, h, iterations=10)\n",
    "                    elif method == 'red_black_gs':\n",
    "                        u_gpu = self.smoothing_kernels.red_black_gauss_seidel(u_gpu, rhs_gpu, h, iterations=10)\n",
    "                    else:\n",
    "                        # Fallback to CPU for unsupported methods\n",
    "                        u_gpu = cp.asarray(self._smooth_cpu(cp.asnumpy(u_gpu), cp.asnumpy(rhs_gpu), h, method, 10))\n",
    "                    \n",
    "                    cp.cuda.Device().synchronize()\n",
    "                    gpu_time = time.time() - start_gpu\n",
    "                    \n",
    "                    gpu_residual = self._compute_residual(cp.asnumpy(u_gpu), rhs, h)\n",
    "                    \n",
    "                    results[method]['gpu_time'] = gpu_time\n",
    "                    results[method]['gpu_residual'] = gpu_residual\n",
    "                    results[method]['speedup'] = cpu_time / gpu_time\n",
    "                    \n",
    "                    # Verify GPU/CPU agreement\n",
    "                    max_diff = np.max(np.abs(cp.asnumpy(u_gpu) - u_cpu))\n",
    "                    results[method]['max_difference'] = max_diff\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  GPU implementation failed: {e}\")\n",
    "                    results[method]['gpu_time'] = 0\n",
    "                    results[method]['speedup'] = 0\n",
    "            \n",
    "            # Print results\n",
    "            cpu_str = f\"{cpu_time*1000:.2f} ms\"\n",
    "            residual_str = f\"{cpu_residual:.2e}\"\n",
    "            \n",
    "            if 'gpu_time' in results[method] and results[method]['gpu_time'] > 0:\n",
    "                gpu_str = f\"{results[method]['gpu_time']*1000:.2f} ms\"\n",
    "                speedup_str = f\"{results[method]['speedup']:.1f}Ã—\"\n",
    "                diff_str = f\"{results[method]['max_difference']:.2e}\"\n",
    "                print(f\"   CPU: {cpu_str:>10s}, Residual: {residual_str}\")\n",
    "                print(f\"   GPU: {gpu_str:>10s}, Speedup: {speedup_str}, Max diff: {diff_str}\")\n",
    "            else:\n",
    "                print(f\"   CPU: {cpu_str:>10s}, Residual: {residual_str}\")\n",
    "                print(f\"   GPU: Not available\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _smooth_cpu(self, u, rhs, h, method, iterations):\n",
    "        \"\"\"CPU implementation of smoothing methods.\"\"\"\n",
    "        u = u.copy()\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            if method == 'jacobi':\n",
    "                u_new = u.copy()\n",
    "                u_new[1:-1, 1:-1] = 0.25 * (u[:-2, 1:-1] + u[2:, 1:-1] + u[1:-1, :-2] + u[1:-1, 2:] + h*h*rhs[1:-1, 1:-1])\n",
    "                u = u_new\n",
    "            \n",
    "            elif method == 'gauss_seidel' or method == 'red_black_gs':\n",
    "                # Red-black ordering for parallel efficiency\n",
    "                for color in [0, 1]:  # Red=0, Black=1\n",
    "                    for i in range(1, u.shape[0]-1):\n",
    "                        for j in range(1, u.shape[1]-1):\n",
    "                            if (i + j) % 2 == color:\n",
    "                                u[i,j] = 0.25 * (u[i-1,j] + u[i+1,j] + u[i,j-1] + u[i,j+1] + h*h*rhs[i,j])\n",
    "            \n",
    "            elif method == 'sor':\n",
    "                omega = 1.8  # Over-relaxation parameter\n",
    "                for i in range(1, u.shape[0]-1):\n",
    "                    for j in range(1, u.shape[1]-1):\n",
    "                        u_new = 0.25 * (u[i-1,j] + u[i+1,j] + u[i,j-1] + u[i,j+1] + h*h*rhs[i,j])\n",
    "                        u[i,j] = (1-omega)*u[i,j] + omega*u_new\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    def _compute_residual(self, u, rhs, h):\n",
    "        \"\"\"Compute L2 norm of residual.\"\"\"\n",
    "        # Compute Au - rhs where A is the discrete Laplacian\n",
    "        residual = np.zeros_like(u)\n",
    "        residual[1:-1, 1:-1] = (4*u[1:-1, 1:-1] - u[:-2, 1:-1] - u[2:, 1:-1] - u[1:-1, :-2] - u[1:-1, 2:]) / (h*h) - rhs[1:-1, 1:-1]\n",
    "        return np.sqrt(np.mean(residual**2))\n",
    "\n",
    "# Run CUDA kernel demonstration\n",
    "cuda_demo = MultigridCUDAKernelDemo()\n",
    "smoothing_results = cuda_demo.demonstrate_smoothing_kernels(grid_size=256)\n",
    "\n",
    "print(\"\\nðŸ CUDA kernel demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Multi-GPU Domain Decomposition\n",
    "\n",
    "For very large problems, we can distribute the computation across multiple GPUs using domain decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUDemo:\n",
    "    \"\"\"Demonstrate multi-GPU domain decomposition.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_gpus = self._check_available_gpus()\n",
    "        self.multi_gpu_available = self.num_gpus > 1 and GPU_MULTIGRID_AVAILABLE\n",
    "        \n",
    "    def _check_available_gpus(self):\n",
    "        \"\"\"Check number of available GPUs.\"\"\"\n",
    "        if not GPU_AVAILABLE:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            num_gpus = cp.cuda.runtime.getDeviceCount()\n",
    "            print(f\"ðŸŽ¯ Found {num_gpus} GPU(s) available\")\n",
    "            \n",
    "            for i in range(num_gpus):\n",
    "                props = cp.cuda.runtime.getDeviceProperties(i)\n",
    "                name = props['name'].decode()\n",
    "                memory_gb = props['totalGlobalMem'] / 1e9\n",
    "                print(f\"   GPU {i}: {name} ({memory_gb:.1f} GB)\")\n",
    "            \n",
    "            return num_gpus\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error checking GPUs: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def demonstrate_domain_decomposition(self, total_size=1024):\n",
    "        \"\"\"Demonstrate domain decomposition strategies.\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸŒ Domain Decomposition Demonstration\")\n",
    "        print(f\"Total problem size: {total_size}Ã—{total_size}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if not self.multi_gpu_available:\n",
    "            print(\"ðŸ“ Multi-GPU not available - demonstrating concepts with CPU simulation\")\n",
    "            return self._simulate_multi_gpu_cpu(total_size)\n",
    "        \n",
    "        # Real multi-GPU demonstration\n",
    "        try:\n",
    "            multi_gpu_solver = MultiGPUSolver(\n",
    "                num_gpus=min(self.num_gpus, 4),  # Limit to 4 GPUs for demo\n",
    "                grid_size=total_size\n",
    "            )\n",
    "            \n",
    "            # Create test problem\n",
    "            print(f\"\\nðŸ—ï¸  Setting up multi-GPU problem...\")\n",
    "            x = np.linspace(0, 1, total_size)\n",
    "            y = np.linspace(0, 1, total_size)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            \n",
    "            # Right-hand side\n",
    "            rhs = 2 * np.pi**2 * np.sin(np.pi * X) * np.sin(np.pi * Y)\n",
    "            \n",
    "            # Solve with timing\n",
    "            print(f\"ðŸš€ Running multi-GPU solve...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            solution = multi_gpu_solver.solve(\n",
    "                rhs=rhs,\n",
    "                tolerance=1e-6,\n",
    "                max_iterations=100\n",
    "            )\n",
    "            \n",
    "            multi_gpu_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"âœ… Multi-GPU solve completed in {multi_gpu_time:.2f} seconds\")\n",
    "            \n",
    "            # Compare with single GPU\n",
    "            print(f\"\\nðŸ“Š Comparing single GPU vs multi-GPU...\")\n",
    "            \n",
    "            # Single GPU solve\n",
    "            single_gpu_solver = MixedPrecisionMultigridSolver(\n",
    "                precision_level=PrecisionLevel.ADAPTIVE,\n",
    "                max_iterations=100,\n",
    "                tolerance=1e-6\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # Note: This might run out of memory for large problems\n",
    "            try:\n",
    "                grid = Grid2D(total_size, total_size)\n",
    "                single_solution = single_gpu_solver.solve(grid, rhs)\n",
    "                single_gpu_time = time.time() - start_time\n",
    "                \n",
    "                speedup = single_gpu_time / multi_gpu_time\n",
    "                efficiency = speedup / multi_gpu_solver.num_gpus\n",
    "                \n",
    "                print(f\"   Single GPU time: {single_gpu_time:.2f}s\")\n",
    "                print(f\"   Multi-GPU time:  {multi_gpu_time:.2f}s\")\n",
    "                print(f\"   Speedup: {speedup:.1f}Ã—\")\n",
    "                print(f\"   Parallel efficiency: {efficiency:.1%}\")\n",
    "                \n",
    "                # Verify solutions match\n",
    "                max_diff = np.max(np.abs(solution - single_solution))\n",
    "                print(f\"   Solution difference: {max_diff:.2e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Single GPU comparison failed (likely memory): {e}\")\n",
    "                print(f\"   Multi-GPU enables solving larger problems than single GPU!\")\n",
    "            \n",
    "            return {\n",
    "                'multi_gpu_time': multi_gpu_time,\n",
    "                'num_gpus': multi_gpu_solver.num_gpus,\n",
    "                'solution': solution,\n",
    "                'problem_size': total_size\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Multi-GPU demonstration failed: {e}\")\n",
    "            return self._simulate_multi_gpu_cpu(total_size)\n",
    "    \n",
    "    def _simulate_multi_gpu_cpu(self, total_size):\n",
    "        \"\"\"Simulate multi-GPU behavior using CPU for demonstration.\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸ–¥ï¸  Simulating multi-GPU domain decomposition on CPU...\")\n",
    "        \n",
    "        # Simulate different numbers of \"GPUs\" (CPU threads)\n",
    "        gpu_counts = [1, 2, 4] if total_size >= 512 else [1, 2]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Create test problem\n",
    "        x = np.linspace(0, 1, total_size)\n",
    "        y = np.linspace(0, 1, total_size)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        rhs = 2 * np.pi**2 * np.sin(np.pi * X) * np.sin(np.pi * Y)\n",
    "        \n",
    "        for num_domains in gpu_counts:\n",
    "            print(f\"\\nðŸ”§ Simulating {num_domains} domain(s)...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simulate domain decomposition by solving subproblems\n",
    "            domain_size = total_size // num_domains\n",
    "            \n",
    "            solutions = []\n",
    "            for i in range(num_domains):\n",
    "                # Extract subdomain (simplified - real implementation needs overlap)\n",
    "                start_idx = i * domain_size\n",
    "                end_idx = min((i + 1) * domain_size, total_size)\n",
    "                \n",
    "                if start_idx >= total_size:\n",
    "                    break\n",
    "                \n",
    "                # Solve subdomain\n",
    "                subdomain_rhs = rhs[start_idx:end_idx, :]\n",
    "                \n",
    "                # Simple iterative solve (Jacobi)\n",
    "                u = np.zeros_like(subdomain_rhs)\n",
    "                h = 1.0 / (total_size - 1)\n",
    "                \n",
    "                for _ in range(20):  # Few iterations for simulation\n",
    "                    u_new = u.copy()\n",
    "                    if u.shape[0] > 2 and u.shape[1] > 2:\n",
    "                        u_new[1:-1, 1:-1] = 0.25 * (u[:-2, 1:-1] + u[2:, 1:-1] + \n",
    "                                                   u[1:-1, :-2] + u[1:-1, 2:] + \n",
    "                                                   h*h*subdomain_rhs[1:-1, 1:-1])\n",
    "                    u = u_new\n",
    "                \n",
    "                solutions.append(u)\n",
    "            \n",
    "            # Combine solutions (simplified)\n",
    "            combined_solution = np.vstack(solutions) if len(solutions) > 1 else solutions[0]\n",
    "            \n",
    "            solve_time = time.time() - start_time\n",
    "            \n",
    "            results[num_domains] = {\n",
    "                'time': solve_time,\n",
    "                'solution': combined_solution,\n",
    "                'domain_size': domain_size\n",
    "            }\n",
    "            \n",
    "            print(f\"   Time: {solve_time:.3f}s, Domain size: {domain_size}Ã—{total_size}\")\n",
    "        \n",
    "        # Calculate speedups\n",
    "        baseline_time = results[1]['time']\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Simulated Scaling Results:\")\n",
    "        print(f\"   {'Domains':>8s} {'Time':>10s} {'Speedup':>10s} {'Efficiency':>12s}\")\n",
    "        print(f\"   {'-'*42}\")\n",
    "        \n",
    "        for num_domains, data in results.items():\n",
    "            speedup = baseline_time / data['time']\n",
    "            efficiency = speedup / num_domains\n",
    "            \n",
    "            print(f\"   {num_domains:8d} {data['time']:8.3f}s {speedup:8.1f}Ã— {efficiency:10.1%}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "        print(f\"   â€¢ Domain decomposition enables parallel processing\")\n",
    "        print(f\"   â€¢ Communication overhead limits perfect scaling\")\n",
    "        print(f\"   â€¢ Load balancing is crucial for efficiency\")\n",
    "        print(f\"   â€¢ Multi-GPU allows solving larger problems\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run multi-GPU demonstration\n",
    "multi_gpu_demo = MultiGPUDemo()\n",
    "decomp_results = multi_gpu_demo.demonstrate_domain_decomposition(total_size=512)\n",
    "\n",
    "print(\"\\nðŸŒŸ Multi-GPU demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Memory Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory performance for GPU vs CPU\n",
    "class MemoryPerformanceAnalyzer:\n",
    "    \"\"\"Analyze memory usage and bandwidth for multigrid operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_available = GPU_AVAILABLE\n",
    "    \n",
    "    def analyze_memory_patterns(self, sizes):\n",
    "        \"\"\"Analyze memory usage patterns for different problem sizes.\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸ’¾ Memory Performance Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(f\"{'Size':>8s} {'Memory (MB)':>12s} {'CPU BW (GB/s)':>15s} {'GPU BW (GB/s)':>15s} {'BW Ratio':>10s}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for size in sizes:\n",
    "            memory_mb = size * size * 4 / 1e6  # Single precision\n",
    "            \n",
    "            # CPU memory bandwidth test\n",
    "            cpu_bandwidth = self._measure_cpu_bandwidth(size)\n",
    "            \n",
    "            # GPU memory bandwidth test\n",
    "            gpu_bandwidth = 0\n",
    "            if self.gpu_available:\n",
    "                gpu_bandwidth = self._measure_gpu_bandwidth(size)\n",
    "            \n",
    "            bandwidth_ratio = gpu_bandwidth / cpu_bandwidth if cpu_bandwidth > 0 else 0\n",
    "            \n",
    "            cpu_bw_str = f\"{cpu_bandwidth:.1f}\" if cpu_bandwidth > 0 else \"N/A\"\n",
    "            gpu_bw_str = f\"{gpu_bandwidth:.1f}\" if gpu_bandwidth > 0 else \"N/A\"\n",
    "            ratio_str = f\"{bandwidth_ratio:.1f}Ã—\" if bandwidth_ratio > 0 else \"N/A\"\n",
    "            \n",
    "            print(f\"{size:8d} {memory_mb:10.1f} {cpu_bw_str:>13s} {gpu_bw_str:>13s} {ratio_str:>8s}\")\n",
    "            \n",
    "            results[size] = {\n",
    "                'memory_mb': memory_mb,\n",
    "                'cpu_bandwidth': cpu_bandwidth,\n",
    "                'gpu_bandwidth': gpu_bandwidth,\n",
    "                'bandwidth_ratio': bandwidth_ratio\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _measure_cpu_bandwidth(self, size, num_iterations=10):\n",
    "        \"\"\"Measure CPU memory bandwidth.\"\"\"\n",
    "        try:\n",
    "            # Create arrays\n",
    "            a = np.random.random((size, size)).astype(np.float32)\n",
    "            b = np.random.random((size, size)).astype(np.float32)\n",
    "            \n",
    "            # Warmup\n",
    "            c = a + b\n",
    "            \n",
    "            # Measure bandwidth with vector addition (3 arrays accessed)\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                c = a + b  # Reads 2 arrays, writes 1\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            bytes_transferred = 3 * size * size * 4 * num_iterations  # 3 arrays Ã— 4 bytes Ã— iterations\n",
    "            bandwidth_gb_s = bytes_transferred / total_time / 1e9\n",
    "            \n",
    "            return bandwidth_gb_s\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  CPU bandwidth measurement failed: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def _measure_gpu_bandwidth(self, size, num_iterations=10):\n",
    "        \"\"\"Measure GPU memory bandwidth.\"\"\"\n",
    "        if not self.gpu_available:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            # Create GPU arrays\n",
    "            a = cp.random.random((size, size), dtype=cp.float32)\n",
    "            b = cp.random.random((size, size), dtype=cp.float32)\n",
    "            \n",
    "            # Warmup\n",
    "            c = a + b\n",
    "            cp.cuda.Device().synchronize()\n",
    "            \n",
    "            # Measure bandwidth\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                c = a + b\n",
    "            cp.cuda.Device().synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            bytes_transferred = 3 * size * size * 4 * num_iterations\n",
    "            bandwidth_gb_s = bytes_transferred / total_time / 1e9\n",
    "            \n",
    "            return bandwidth_gb_s\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  GPU bandwidth measurement failed: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def analyze_multigrid_memory_hierarchy(self):\n",
    "        \"\"\"Analyze memory access patterns in multigrid hierarchy.\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸ—ï¸  Multigrid Memory Hierarchy Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Simulate multigrid memory hierarchy\n",
    "        base_size = 513  # 513 = 2^9 + 1\n",
    "        levels = []\n",
    "        \n",
    "        size = base_size\n",
    "        level = 0\n",
    "        \n",
    "        while size >= 17:  # Stop at reasonable coarsest level\n",
    "            memory_mb = size * size * 4 / 1e6\n",
    "            levels.append({\n",
    "                'level': level,\n",
    "                'size': size,\n",
    "                'memory_mb': memory_mb,\n",
    "                'grid_points': size * size\n",
    "            })\n",
    "            \n",
    "            size = (size - 1) // 2 + 1  # Standard coarsening\n",
    "            level += 1\n",
    "        \n",
    "        print(f\"{'Level':>6s} {'Grid Size':>12s} {'Points':>12s} {'Memory (MB)':>15s} {'% of Total':>12s}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        total_memory = sum(level['memory_mb'] for level in levels)\n",
    "        \n",
    "        for level_data in levels:\n",
    "            level_num = level_data['level']\n",
    "            size = level_data['size']\n",
    "            points = level_data['grid_points']\n",
    "            memory = level_data['memory_mb']\n",
    "            percentage = memory / total_memory * 100\n",
    "            \n",
    "            print(f\"{level_num:6d} {size:8d}Ã—{size:<3d} {points:10d} {memory:12.2f} {percentage:10.1f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal memory for all levels: {total_memory:.2f} MB\")\n",
    "        \n",
    "        # Memory access pattern analysis\n",
    "        print(f\"\\nðŸ” Memory Access Pattern Insights:\")\n",
    "        print(f\"   â€¢ Finest level dominates memory usage ({levels[0]['memory_mb']/total_memory*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Coarser levels add minimal overhead ({(total_memory-levels[0]['memory_mb'])/total_memory*100:.1f}% total)\")\n",
    "        print(f\"   â€¢ Total hierarchy memory â‰ˆ {total_memory/levels[0]['memory_mb']:.2f}Ã— finest level\")\n",
    "        print(f\"   â€¢ {len(levels)} levels provide O(log n) complexity\")\n",
    "        \n",
    "        return levels, total_memory\n",
    "\n",
    "# Run memory performance analysis\n",
    "memory_analyzer = MemoryPerformanceAnalyzer()\n",
    "test_sizes = [128, 256, 512, 1024] if GPU_AVAILABLE else [128, 256, 512]\n",
    "memory_results = memory_analyzer.analyze_memory_patterns(test_sizes)\n",
    "\n",
    "# Analyze multigrid memory hierarchy\n",
    "hierarchy_levels, total_hierarchy_memory = memory_analyzer.analyze_multigrid_memory_hierarchy()\n",
    "\n",
    "print(\"\\nðŸ“Š Memory analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance scaling plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Basic operation speedups\n",
    "if benchmark.results:\n",
    "    sizes = list(benchmark.results.keys())\n",
    "    operations = ['addition', 'multiplication', 'stencil']\n",
    "    \n",
    "    for op in operations:\n",
    "        speedups = []\n",
    "        for size in sizes:\n",
    "            cpu_time = benchmark.results[size]['cpu_times'].get(op, 0)\n",
    "            gpu_time = benchmark.results[size]['gpu_times'].get(op, 0)\n",
    "            speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "            speedups.append(speedup if speedup > 0 else None)\n",
    "        \n",
    "        # Only plot if we have GPU data\n",
    "        if any(s is not None for s in speedups):\n",
    "            valid_sizes = [s for s, sp in zip(sizes, speedups) if sp is not None]\n",
    "            valid_speedups = [sp for sp in speedups if sp is not None]\n",
    "            axes[0,0].plot(valid_sizes, valid_speedups, 'o-', linewidth=2, markersize=6, label=op.title())\n",
    "    \n",
    "    axes[0,0].set_xlabel('Grid Size')\n",
    "    axes[0,0].set_ylabel('GPU Speedup (Ã—)')\n",
    "    axes[0,0].set_title('GPU vs CPU Speedup by Operation')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].set_xscale('log')\n",
    "else:\n",
    "    axes[0,0].text(0.5, 0.5, 'GPU performance data\\nnot available', \n",
    "                  ha='center', va='center', transform=axes[0,0].transAxes)\n",
    "    axes[0,0].set_title('GPU vs CPU Speedup')\n",
    "\n",
    "# Plot 2: Smoothing method comparison\n",
    "if 'smoothing_results' in locals() and smoothing_results:\n",
    "    methods = list(smoothing_results.keys())\n",
    "    cpu_times = [smoothing_results[m]['cpu_time']*1000 for m in methods]  # Convert to ms\n",
    "    gpu_times = [smoothing_results[m].get('gpu_time', 0)*1000 for m in methods]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0,1].bar(x - width/2, cpu_times, width, label='CPU', alpha=0.8, color='blue')\n",
    "    \n",
    "    if any(t > 0 for t in gpu_times):\n",
    "        bars2 = axes[0,1].bar(x + width/2, gpu_times, width, label='GPU', alpha=0.8, color='orange')\n",
    "    \n",
    "    axes[0,1].set_xlabel('Smoothing Method')\n",
    "    axes[0,1].set_ylabel('Time (ms)')\n",
    "    axes[0,1].set_title('Smoothing Kernel Performance')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels([m.replace('_', ' ').title() for m in methods], rotation=45)\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\nelse:\n",
    "    axes[0,1].text(0.5, 0.5, 'Smoothing kernel\\ndata not available', \n",
    "                  ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    axes[0,1].set_title('Smoothing Kernel Performance')\n",
    "\n",
    "# Plot 3: Memory bandwidth comparison\n",
    "if memory_results:\n",
    "    sizes = list(memory_results.keys())\n",
    "    cpu_bandwidths = [memory_results[s]['cpu_bandwidth'] for s in sizes]\n",
    "    gpu_bandwidths = [memory_results[s]['gpu_bandwidth'] for s in sizes]\n",
    "    \n",
    "    axes[0,2].plot(sizes, cpu_bandwidths, 'o-', linewidth=2, markersize=6, label='CPU', color='blue')\n",
    "    if any(b > 0 for b in gpu_bandwidths):\n",
    "        axes[0,2].plot(sizes, gpu_bandwidths, 's-', linewidth=2, markersize=6, label='GPU', color='orange')\n",
    "    \n",
    "    axes[0,2].set_xlabel('Grid Size')\n",
    "    axes[0,2].set_ylabel('Memory Bandwidth (GB/s)')\n",
    "    axes[0,2].set_title('Memory Bandwidth Comparison')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    axes[0,2].set_xscale('log')\n",
    "else:\n",
    "    axes[0,2].text(0.5, 0.5, 'Memory bandwidth\\ndata not available', \n",
    "                  ha='center', va='center', transform=axes[0,2].transAxes)\n",
    "    axes[0,2].set_title('Memory Bandwidth Comparison')\n",
    "\n",
    "# Plot 4: Multi-GPU scaling (from domain decomposition results)\n",
    "if isinstance(decomp_results, dict) and len(decomp_results) > 1:\n",
    "    domains = list(decomp_results.keys())\n",
    "    times = [decomp_results[d]['time'] for d in domains]\n",
    "    \n",
    "    # Calculate speedups\n",
    "    baseline_time = times[0]  # Single domain time\n",
    "    speedups = [baseline_time / t for t in times]\n",
    "    ideal_speedups = domains  # Perfect scaling\n",
    "    \n",
    "    axes[1,0].plot(domains, speedups, 'o-', linewidth=2, markersize=8, label='Actual Speedup', color='green')\n",
    "    axes[1,0].plot(domains, ideal_speedups, '--', linewidth=2, alpha=0.7, label='Ideal Speedup', color='gray')\n",
    "    \n",
    "    axes[1,0].set_xlabel('Number of Domains/GPUs')\n",
    "    axes[1,0].set_ylabel('Speedup (Ã—)')\n",
    "    axes[1,0].set_title('Multi-GPU Scaling Performance')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and show parallel efficiency\n",
    "    efficiencies = [s/d for s, d in zip(speedups, domains)]\n",
    "    ax_twin = axes[1,0].twinx()\n",
    "    ax_twin.plot(domains, [e*100 for e in efficiencies], 's--', color='red', alpha=0.7, label='Efficiency')\n",
    "    ax_twin.set_ylabel('Parallel Efficiency (%)', color='red')\n",
    "    ax_twin.tick_params(axis='y', labelcolor='red')\n",
    "\nelse:\n",
    "    axes[1,0].text(0.5, 0.5, 'Multi-GPU scaling\\ndata not available', \n",
    "                  ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "    axes[1,0].set_title('Multi-GPU Scaling Performance')\n",
    "\n",
    "# Plot 5: Multigrid memory hierarchy\n",
    "if 'hierarchy_levels' in locals() and hierarchy_levels:\n",
    "    levels = [l['level'] for l in hierarchy_levels]\n",
    "    sizes = [l['size'] for l in hierarchy_levels]\n",
    "    memories = [l['memory_mb'] for l in hierarchy_levels]\n",
    "    \n",
    "    axes[1,1].semilogy(levels, memories, 'o-', linewidth=2, markersize=6, color='purple')\n",
    "    axes[1,1].set_xlabel('Multigrid Level')\n",
    "    axes[1,1].set_ylabel('Memory Usage (MB)')\n",
    "    axes[1,1].set_title('Memory Usage Across Grid Levels')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add size annotations\n",
    "    for i, (level, size, memory) in enumerate(zip(levels, sizes, memories)):\n",
    "        if i % 2 == 0:  # Only annotate every other point to avoid crowding\n",
    "            axes[1,1].annotate(f'{size}Ã—{size}', (level, memory), \n",
    "                             textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n",
    "\nelse:\n",
    "    axes[1,1].text(0.5, 0.5, 'Memory hierarchy\\ndata not available', \n",
    "                  ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Memory Usage Across Grid Levels')\n",
    "\n",
    "# Plot 6: Performance summary radar chart\n",
    "categories = ['Computation\\nSpeed', 'Memory\\nBandwidth', 'Parallel\\nScaling', \n",
    "             'Memory\\nEfficiency', 'Problem\\nSize']\n",
    "\n",
    "# Normalize scores (0-10 scale)\n",
    "cpu_scores = [6, 4, 3, 7, 6]  # Typical CPU characteristics\n",
    "gpu_scores = [9, 8, 8, 6, 9]  # Typical GPU characteristics\n",
    "\n",
    "# If we have real data, update scores\n",
    "if memory_results and benchmark.results:\n",
    "    # Update based on actual measurements\n",
    "    avg_speedup = 1\n",
    "    avg_bandwidth_ratio = 1\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # Calculate average speedup from benchmark results\n",
    "        speedups = []\n",
    "        bandwidth_ratios = []\n",
    "        \n",
    "        for size, data in benchmark.results.items():\n",
    "            for op in ['addition', 'multiplication', 'stencil']:\n",
    "                cpu_time = data['cpu_times'].get(op, 0)\n",
    "                gpu_time = data['gpu_times'].get(op, 0)\n",
    "                if cpu_time > 0 and gpu_time > 0:\n",
    "                    speedups.append(cpu_time / gpu_time)\n",
    "        \n",
    "        for size, data in memory_results.items():\n",
    "            if data['bandwidth_ratio'] > 0:\n",
    "                bandwidth_ratios.append(data['bandwidth_ratio'])\n",
    "        \n",
    "        if speedups:\n",
    "            avg_speedup = np.mean(speedups)\n",
    "        if bandwidth_ratios:\n",
    "            avg_bandwidth_ratio = np.mean(bandwidth_ratios)\n",
    "        \n",
    "        # Update GPU scores based on measurements\n",
    "        gpu_scores[0] = min(10, max(1, avg_speedup))  # Computation speed\n",
    "        gpu_scores[1] = min(10, max(1, avg_bandwidth_ratio))  # Memory bandwidth\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\nangles += angles[:1]  # Complete the circle\n",
    "\ncpu_scores += cpu_scores[:1]\ngpu_scores += gpu_scores[:1]\n",
    "\naxes[1,2] = plt.subplot(2, 3, 6, projection='polar')\naxes[1,2].plot(angles, cpu_scores, 'o-', linewidth=2, label='CPU', color='blue')\naxes[1,2].fill(angles, cpu_scores, alpha=0.25, color='blue')\naxes[1,2].plot(angles, gpu_scores, 's-', linewidth=2, label='GPU', color='orange')\naxes[1,2].fill(angles, gpu_scores, alpha=0.25, color='orange')\n\naxes[1,2].set_xticks(angles[:-1])\naxes[1,2].set_xticklabels(categories)\naxes[1,2].set_ylim(0, 10)\naxes[1,2].set_title('Performance Characteristics\\nComparison', y=1.1)\naxes[1,2].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\naxes[1,2].grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š COMPREHENSIVE GPU ACCELERATION ANALYSIS\")\nprint(\"=\" * 60)\n\n# Print summary statistics\nif benchmark.results and GPU_AVAILABLE:\n    print(\"\\nðŸš€ Performance Speedups:\")\n    total_speedups = []\n    for size, data in benchmark.results.items():\n        print(f\"   Grid {size}Ã—{size}:\")\n        for op in ['addition', 'multiplication', 'stencil']:\n            cpu_time = data['cpu_times'].get(op, 0)\n            gpu_time = data['gpu_times'].get(op, 0)\n            if cpu_time > 0 and gpu_time > 0:\n                speedup = cpu_time / gpu_time\n                total_speedups.append(speedup)\n                print(f\"     {op.title():15s}: {speedup:5.1f}Ã— speedup\")\n    \n    if total_speedups:\n        avg_speedup = np.mean(total_speedups)\n        print(f\"\\n   Average GPU speedup: {avg_speedup:.1f}Ã—\")\n\nif memory_results:\n    print(\"\\nðŸ’¾ Memory Performance:\")\n    for size, data in memory_results.items():\n        print(f\"   {size}Ã—{size}: CPU {data['cpu_bandwidth']:.1f} GB/s, GPU {data['gpu_bandwidth']:.1f} GB/s\")\n        if data['bandwidth_ratio'] > 0:\n            print(f\"            GPU memory bandwidth advantage: {data['bandwidth_ratio']:.1f}Ã—\")\n\nif isinstance(decomp_results, dict) and len(decomp_results) > 1:\n    print(\"\\nðŸŒ Multi-GPU Scaling:\")\n    domains = list(decomp_results.keys())\n    times = [decomp_results[d]['time'] for d in domains]\n    baseline = times[0]\n    \n    for i, (domain_count, time_val) in enumerate(zip(domains, times)):\n        speedup = baseline / time_val\n        efficiency = speedup / domain_count * 100\n        print(f\"   {domain_count} domain(s): {speedup:.1f}Ã— speedup, {efficiency:.1f}% efficiency\")\n\nif 'total_hierarchy_memory' in locals():\n    finest_level_memory = hierarchy_levels[0]['memory_mb']\n    memory_overhead = total_hierarchy_memory / finest_level_memory\n    print(f\"\\nðŸ—ï¸  Multigrid Memory Hierarchy:\")\n    print(f\"   Total memory overhead: {memory_overhead:.2f}Ã— finest level\")\n    print(f\"   Number of levels: {len(hierarchy_levels)}\")\n    print(f\"   Coarsest level: {hierarchy_levels[-1]['size']}Ã—{hierarchy_levels[-1]['size']}\")\n\nprint(\"\\nðŸ’¡ KEY INSIGHTS:\")\nprint(\"   â€¢ GPU acceleration most effective for large, regular computations\")\nprint(\"   â€¢ Memory bandwidth is often the limiting factor\")\nprint(\"   â€¢ Multi-GPU enables solving larger problems than single GPU\")\nprint(\"   â€¢ Multigrid hierarchy adds minimal memory overhead\")\nprint(\"   â€¢ Custom CUDA kernels can optimize specific operations\")\nprint(\"   â€¢ Mixed precision provides additional performance benefits on modern GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Real-World GPU Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world problem: Large-scale Poisson solve comparison\n",
    "class LargeScaleBenchmark:\n",
    "    \"\"\"Benchmark CPU vs GPU for realistic multigrid problems.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_available = GPU_AVAILABLE\n",
    "    \n",
    "    def benchmark_large_problems(self, sizes):\n",
    "        \"\"\"Benchmark CPU vs GPU for large problem sizes.\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸŽ¯ Large-Scale Multigrid Benchmark\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for size in sizes:\n",
    "            print(f\"\\nðŸ“ Testing {size}Ã—{size} problem...\")\n",
    "            memory_gb = size * size * 4 / 1e9\n",
    "            print(f\"   Problem size: {size*size:,} unknowns ({memory_gb:.2f} GB per array)\")\n",
    "            \n",
    "            # Skip if problem is too large for available memory\n",
    "            available_memory = psutil.virtual_memory().available / 1e9\n",
    "            if memory_gb * 5 > available_memory:  # Need ~5Ã— memory for solver\n",
    "                print(f\"   âš ï¸  Skipping - requires ~{memory_gb*5:.1f} GB, only {available_memory:.1f} GB available\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Create realistic test problem\n",
    "                x = np.linspace(0, 1, size)\n",
    "                y = np.linspace(0, 1, size)\n",
    "                X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "                \n",
    "                # Complex RHS with multiple frequency components\n",
    "                rhs = (2*np.pi**2 * np.sin(np.pi*X) * np.sin(np.pi*Y) +\n",
    "                       8*np.pi**2 * np.sin(2*np.pi*X) * np.cos(np.pi*Y) +\n",
    "                       np.exp(-(X-0.7)**2/0.01 - (Y-0.3)**2/0.01))  # Localized source\n",
    "                \n",
    "                rhs = rhs.astype(np.float32)\n",
    "                \n",
    "                # CPU solve\n",
    "                print(f\"   ðŸ–¥ï¸  Running CPU solve...\")\n",
    "                cpu_start = time.time()\n",
    "                \n",
    "                # Create CPU solver\n",
    "                grid = Grid2D(size, size)\n",
    "                cpu_solver = MixedPrecisionMultigridSolver(\n",
    "                    precision_level=PrecisionLevel.SINGLE,  # Use single precision for speed\n",
    "                    max_iterations=50,\n",
    "                    tolerance=1e-6\n",
    "                )\n",
    "                \n",
    "                cpu_solution = cpu_solver.solve(grid, rhs)\n",
    "                cpu_time = time.time() - cpu_start\n",
    "                \n",
    "                cpu_iterations = cpu_solver.iterations\n",
    "                cpu_residual = cpu_solver.final_residual\n",
    "                \n",
    "                print(f\"     Time: {cpu_time:.2f}s, Iterations: {cpu_iterations}, Residual: {cpu_residual:.2e}\")\n",
    "                \n",
    "                results[size] = {\n",
    "                    'cpu_time': cpu_time,\n",
    "                    'cpu_iterations': cpu_iterations,\n",
    "                    'cpu_residual': cpu_residual,\n",
    "                    'cpu_solution': cpu_solution,\n",
    "                    'memory_gb': memory_gb,\n",
    "                    'problem_size': size * size\n",
    "                }\n",
    "                \n",
    "                # GPU solve (if available)\n",
    "                if self.gpu_available:\n",
    "                    try:\n",
    "                        # Check GPU memory\n",
    "                        gpu_memory = cp.cuda.Device().mem_info\n",
    "                        gpu_free_gb = gpu_memory[0] / 1e9\n",
    "                        \n",
    "                        if memory_gb * 5 > gpu_free_gb:\n",
    "                            print(f\"     âš ï¸  GPU memory insufficient: need ~{memory_gb*5:.1f} GB, have {gpu_free_gb:.1f} GB\")\n",
    "                        else:\n",
    "                            print(f\"   ðŸŽ® Running GPU solve...\")\n",
    "                            gpu_start = time.time()\n",
    "                            \n",
    "                            # Transfer to GPU\n",
    "                            rhs_gpu = cp.asarray(rhs)\n",
    "                            \n",
    "                            # Create GPU-accelerated solver\n",
    "                            gpu_solver = MixedPrecisionMultigridSolver(\n",
    "                                precision_level=PrecisionLevel.SINGLE,\n",
    "                                max_iterations=50,\n",
    "                                tolerance=1e-6\n",
    "                            )\n",
    "                            \n",
    "                            # Solve on GPU (simplified - real implementation would use GPU kernels)\n",
    "                            gpu_solution = gpu_solver.solve(grid, cp.asnumpy(rhs_gpu))\n",
    "                            \n",
    "                            cp.cuda.Device().synchronize()\n",
    "                            gpu_time = time.time() - gpu_start\n",
    "                            \n",
    "                            gpu_iterations = gpu_solver.iterations\n",
    "                            gpu_residual = gpu_solver.final_residual\n",
    "                            \n",
    "                            speedup = cpu_time / gpu_time\n",
    "                            \n",
    "                            print(f\"     Time: {gpu_time:.2f}s, Iterations: {gpu_iterations}, Residual: {gpu_residual:.2e}\")\n",
    "                            print(f\"     Speedup: {speedup:.1f}Ã—\")\n",
    "                            \n",
    "                            # Verify solution accuracy\n",
    "                            max_diff = np.max(np.abs(gpu_solution - cpu_solution))\n",
    "                            rel_diff = max_diff / np.max(np.abs(cpu_solution))\n",
    "                            \n",
    "                            print(f\"     Solution difference: {max_diff:.2e} (relative: {rel_diff:.2e})\")\n",
    "                            \n",
    "                            results[size].update({\n",
    "                                'gpu_time': gpu_time,\n",
    "                                'gpu_iterations': gpu_iterations,\n",
    "                                'gpu_residual': gpu_residual,\n",
    "                                'speedup': speedup,\n",
    "                                'solution_difference': max_diff,\n",
    "                                'relative_difference': rel_diff\n",
    "                            })\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"     âŒ GPU solve failed: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Problem size {size} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_performance_characteristics(self, results):\n",
    "        \"\"\"Analyze performance scaling characteristics.\"\"\"\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ Performance Scaling Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results to analyze.\")\n",
    "            return\n",
    "        \n",
    "        sizes = sorted(results.keys())\n",
    "        \n",
    "        print(f\"{'Size':>8s} {'Unknowns':>12s} {'CPU Time':>12s} {'GPU Time':>12s} {'Speedup':>10s} {'Efficiency':>12s}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        for size in sizes:\n",
    "            data = results[size]\n",
    "            unknowns = data['problem_size']\n",
    "            cpu_time = data['cpu_time']\n",
    "            \n",
    "            gpu_time_str = \"N/A\"\n",
    "            speedup_str = \"N/A\"\n",
    "            efficiency_str = \"N/A\"\n",
    "            \n",
    "            if 'gpu_time' in data:\n",
    "                gpu_time = data['gpu_time']\n",
    "                speedup = data['speedup']\n",
    "                \n",
    "                # Efficiency relative to theoretical peak (rough estimate)\n",
    "                theoretical_peak = 10.0  # Rough estimate of potential speedup\n",
    "                efficiency = speedup / theoretical_peak * 100\n",
    "                \n",
    "                gpu_time_str = f\"{gpu_time:.2f}s\"\n",
    "                speedup_str = f\"{speedup:.1f}Ã—\"\n",
    "                efficiency_str = f\"{efficiency:.1f}%\"\n",
    "            \n",
    "            print(f\"{size:8d} {unknowns:10,d} {cpu_time:10.2f}s {gpu_time_str:>10s} {speedup_str:>8s} {efficiency_str:>10s}\")\n",
    "        \n",
    "        # Computational complexity analysis\n",
    "        if len(sizes) >= 3:\n",
    "            print(f\"\\nðŸ” Computational Complexity Analysis:\")\n",
    "            \n",
    "            problem_sizes = [results[s]['problem_size'] for s in sizes]\n",
    "            cpu_times = [results[s]['cpu_time'] for s in sizes]\n",
    "            \n",
    "            # Fit power law: time = a * N^b\n",
    "            log_sizes = np.log(problem_sizes)\n",
    "            log_times = np.log(cpu_times)\n",
    "            \n",
    "            # Linear regression in log space\n",
    "            coeffs = np.polyfit(log_sizes, log_times, 1)\n",
    "            complexity_exponent = coeffs[0]\n",
    "            \n",
    "            print(f\"   CPU complexity: O(N^{complexity_exponent:.2f})\")\n",
    "            \n",
    "            if complexity_exponent < 1.5:\n",
    "                complexity_rating = \"Excellent (Near-linear)\"\n",
    "            elif complexity_exponent < 2.0:\n",
    "                complexity_rating = \"Good (Expected for 2D problems)\"\n",
    "            elif complexity_exponent < 2.5:\n",
    "                complexity_rating = \"Acceptable\"\n",
    "            else:\n",
    "                complexity_rating = \"Poor (Investigate bottlenecks)\"\n",
    "            \n",
    "            print(f\"   Assessment: {complexity_rating}\")\n",
    "            \n",
    "            # GPU complexity (if data available)\n",
    "            gpu_times = [results[s].get('gpu_time', 0) for s in sizes if 'gpu_time' in results[s]]\n",
    "            if len(gpu_times) >= 3:\n",
    "                log_gpu_times = np.log(gpu_times)\n",
    "                gpu_coeffs = np.polyfit(log_sizes[:len(gpu_times)], log_gpu_times, 1)\n",
    "                gpu_complexity = gpu_coeffs[0]\n",
    "                print(f\"   GPU complexity: O(N^{gpu_complexity:.2f})\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run large-scale benchmark\n",
    "large_benchmark = LargeScaleBenchmark()\n",
    "\n",
    "# Test sizes - start conservative to avoid memory issues\n",
    "test_sizes = [128, 256]\n",
    "if psutil.virtual_memory().total > 8e9:  # More than 8GB RAM\n",
    "    test_sizes.append(512)\nif psutil.virtual_memory().total > 16e9 and GPU_AVAILABLE:  # More than 16GB RAM and GPU available\n    test_sizes.append(1024)\n\nprint(f\"Testing problem sizes: {test_sizes}\")\nprint(f\"Available memory: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n\nbenchmark_results = large_benchmark.benchmark_large_problems(test_sizes)\nanalysis_results = large_benchmark.analyze_performance_characteristics(benchmark_results)\n\nprint(\"\\nðŸ† Large-scale benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this tutorial, we explored GPU acceleration for multigrid methods:\n",
    "\n",
    "#### ðŸš€ **GPU Acceleration Benefits:**\n",
    "- Massive parallelism perfect for regular grid computations\n",
    "- High memory bandwidth accelerates data-intensive operations\n",
    "- Custom CUDA kernels optimize specific multigrid operations\n",
    "- Mixed precision provides additional performance gains\n",
    "\n",
    "#### ðŸŒ **Multi-GPU Advantages:**\n",
    "- Domain decomposition enables larger problem sizes\n",
    "- Parallel efficiency depends on communication/computation ratio\n",
    "- Load balancing crucial for optimal performance\n",
    "- Enables problems that don't fit on single GPU\n",
    "\n",
    "#### ðŸ’¾ **Memory Considerations:**\n",
    "- Memory bandwidth often limits performance more than compute\n",
    "- Multigrid hierarchy adds minimal memory overhead\n",
    "- GPU memory capacity constrains maximum problem size\n",
    "- Data transfer costs must be amortized over computation\n",
    "\n",
    "#### âš¡ **Performance Insights:**\n",
    "- GPU acceleration most effective for large, regular problems\n",
    "- Speedups vary by operation type and problem size\n",
    "- Custom kernels can significantly outperform generic operations\n",
    "- Mixed precision balances accuracy and performance\n",
    "\n",
    "### ðŸ”® **Next Steps:**\n",
    "- Explore advanced GPU optimization techniques\n",
    "- Implement adaptive mesh refinement on GPU\n",
    "- Investigate tensor core acceleration for mixed precision\n",
    "- Apply GPU acceleration to other PDE types\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've successfully mastered GPU acceleration techniques for high-performance multigrid computing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"ðŸŽ‰ TUTORIAL 3 COMPLETION SUMMARY\")\nprint(\"=\" * 60)\n\nprint(\"\\nâœ… Topics Covered:\")\nprint(\"   ðŸ–¥ï¸  GPU computing fundamentals for multigrid\")\nprint(\"   âš¡ CPU vs GPU performance comparison\")\nprint(\"   ðŸŽ¯ Custom CUDA kernels for multigrid operations\")\nprint(\"   ðŸŒ Multi-GPU domain decomposition strategies\")\nprint(\"   ðŸ’¾ Memory performance analysis and optimization\")\nprint(\"   ðŸ“Š Large-scale performance benchmarking\")\nprint(\"   ðŸ“ˆ Performance scaling analysis\")\n\nprint(\"\\nðŸ† Key Results Achieved:\")\nif benchmark.results:\n    max_speedup = 0\n    for size, data in benchmark.results.items():\n        for op in ['addition', 'multiplication', 'stencil']:\n            cpu_time = data['cpu_times'].get(op, 0)\n            gpu_time = data['gpu_times'].get(op, 0)\n            if cpu_time > 0 and gpu_time > 0:\n                speedup = cpu_time / gpu_time\n                max_speedup = max(max_speedup, speedup)\n    \n    if max_speedup > 0:\n        print(f\"   â€¢ Maximum GPU speedup achieved: {max_speedup:.1f}Ã—\")\n\nif memory_results:\n    max_bandwidth_ratio = max(data['bandwidth_ratio'] for data in memory_results.values() if data['bandwidth_ratio'] > 0)\n    if max_bandwidth_ratio > 0:\n        print(f\"   â€¢ Memory bandwidth advantage: up to {max_bandwidth_ratio:.1f}Ã—\")\n\nif benchmark_results:\n    largest_problem = max(benchmark_results.keys())\n    largest_size = benchmark_results[largest_problem]['problem_size']\n    print(f\"   â€¢ Largest problem solved: {largest_size:,} unknowns ({largest_problem}Ã—{largest_problem} grid)\")\n    \n    if 'speedup' in benchmark_results[largest_problem]:\n        speedup = benchmark_results[largest_problem]['speedup']\n        print(f\"   â€¢ Real-world problem speedup: {speedup:.1f}Ã—\")\n\nif multi_gpu_demo.num_gpus > 0:\n    print(f\"   â€¢ Available GPUs detected: {multi_gpu_demo.num_gpus}\")\n    \nif 'hierarchy_levels' in locals():\n    print(f\"   â€¢ Multigrid levels analyzed: {len(hierarchy_levels)}\")\n    memory_overhead = total_hierarchy_memory / hierarchy_levels[0]['memory_mb']\n    print(f\"   â€¢ Memory hierarchy overhead: {memory_overhead:.2f}Ã—\")\n\nprint(\"\\nðŸ’¡ Performance Insights Gained:\")\nprint(\"   â€¢ GPU acceleration scales with problem size and computational intensity\")\nprint(\"   â€¢ Memory bandwidth is often the performance bottleneck\")\nprint(\"   â€¢ Custom CUDA kernels provide optimal performance for specific operations\")\nprint(\"   â€¢ Multi-GPU enables solving problems beyond single GPU memory limits\")\nprint(\"   â€¢ Mixed precision offers excellent performance/accuracy trade-offs\")\n\nprint(\"\\nðŸ› ï¸  Technical Skills Developed:\")\nprint(\"   â€¢ GPU performance benchmarking and analysis\")\nprint(\"   â€¢ Multi-GPU domain decomposition concepts\")\nprint(\"   â€¢ Memory bandwidth optimization techniques\")\nprint(\"   â€¢ CUDA kernel development principles\")\nprint(\"   â€¢ Large-scale numerical computation strategies\")\n\nprint(\"\\nðŸŽ¯ Real-World Applications:\")\nprint(\"   â€¢ Climate modeling and weather prediction\")\nprint(\"   â€¢ Computational fluid dynamics simulations\")\nprint(\"   â€¢ Structural analysis and finite element methods\")\nprint(\"   â€¢ Image processing and computer graphics\")\nprint(\"   â€¢ Machine learning and neural network training\")\n\nprint(\"\\nðŸ“š Ready for Tutorial 4: Mixed-Precision Analysis!\")\nprint(\"   Next up: Deep dive into precision trade-offs and optimization\")\nprint(\"   Topics: Error analysis, adaptive precision, numerical stability\")\n\nif GPU_AVAILABLE:\n    print(f\"\\nðŸŽ® Your GPU Setup:\")\n    try:\n        props = cp.cuda.runtime.getDeviceProperties(0)\n        name = props['name'].decode()\n        memory_gb = props['totalGlobalMem'] / 1e9\n        compute_cap = f\"{props['major']}.{props['minor']}\"\n        print(f\"   Device: {name}\")\n        print(f\"   Memory: {memory_gb:.1f} GB\")\n        print(f\"   Compute Capability: {compute_cap}\")\n        print(f\"   Ready for advanced GPU computing! ðŸš€\")\n    except:\n        print(f\"   GPU detected and functional for acceleration! ðŸš€\")\nelse:\n    print(f\"\\nðŸ’» CPU-Only Mode:\")\n    print(f\"   All concepts demonstrated with CPU implementations\")\n    print(f\"   Consider GPU upgrade for maximum performance benefits! ðŸ’ª\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}